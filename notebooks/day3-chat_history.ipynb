{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c5aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables.base import RunnableParallel\n",
    "\n",
    "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c85a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant -- Gemini 2.5 flash lite.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e6db263",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddc9597",
   "metadata": {},
   "source": [
    "### **Simple Buffer Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f05a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | llm\n",
    "\n",
    "chain_with_buffer_memory = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_variables=\"input\",\n",
    "    # output_variables=None,\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0364973f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi John! It's nice to meet you. How can I help you today?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_buffer_memory.invoke(\n",
    "    {\"input\": \"My name is John.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"simple_memory_llm\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "945c2d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is John.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_buffer_memory.invoke(\n",
    "    {\"input\": \"What is my name?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"simple_memory_llm\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc8aa478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: My name is John.\n",
      "AI: Hi John! It's nice to meet you. How can I help you today?\n",
      "Human: What is my name?\n",
      "AI: Your name is John.\n"
     ]
    }
   ],
   "source": [
    "print(get_session_history(\"simple_memory_llm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa77f85e",
   "metadata": {},
   "source": [
    "### **k buffer window**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5df583d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.utils import trim_messages\n",
    "\n",
    "sample_messages = [\n",
    "    HumanMessage(\"Hi, I'm looking for a book on quantum physics.\"),\n",
    "    AIMessage(\"Hello! We have several options. Are you looking for something for beginners or a more advanced text?\"),\n",
    "    HumanMessage(\"I'm a beginner, so something introductory would be great.\"),\n",
    "    AIMessage(\"I recommend 'Quantum Enigma' by Bruce Rosenblum. It explains complex ideas in an easy-to-understand way.\"),\n",
    "    HumanMessage(\"That sounds perfect. Do you have it in stock?\"),\n",
    "    AIMessage(\"Yes, we do. I can have a copy held for you at the front desk.\"),\n",
    "    HumanMessage(\"Thank you so much!\"),\n",
    "    AIMessage(\"You're welcome! Let me know if you need anything else.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ac14b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Yes, we do. I can have a copy held for you at the front desk.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Thank you so much!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"You're welcome! Let me know if you need anything else.\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep k last tokens: `token_counter=BaseLanguageModel`\n",
    "trim_messages(\n",
    "    sample_messages,\n",
    "    max_tokens=50,\n",
    "    strategy=\"last\",\n",
    "    token_counter=llm,\n",
    "    # start_on=\"human\",\n",
    "    include_system=False,\n",
    "    allow_partial=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da7bd2f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='That sounds perfect. Do you have it in stock?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Yes, we do. I can have a copy held for you at the front desk.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Thank you so much!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"You're welcome! Let me know if you need anything else.\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep k last messages: `token_counter=len`\n",
    "trim_messages(\n",
    "    sample_messages,\n",
    "    max_tokens=4,\n",
    "    strategy=\"last\",\n",
    "    token_counter=len,\n",
    "    # start_on=\"human\",\n",
    "    include_system=False,\n",
    "    allow_partial=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df4a881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_chat_history(x):\n",
    "    trimmed_messages = trim_messages(\n",
    "        x['chat_history'],\n",
    "        max_tokens=2,\n",
    "        strategy=\"last\",\n",
    "        token_counter=len,\n",
    "        include_system=False,\n",
    "        allow_partial=False\n",
    "    )\n",
    "    return trimmed_messages\n",
    "\n",
    "# {\"input\": ..., \"chat_history\": messeges]}\n",
    "# --> {\"input\": ..., \"chat_history\": trimmed_messeges}\n",
    "chain = RunnablePassthrough.assign(chat_history=trim_chat_history) | prompt_template | llm\n",
    "\n",
    "chain_with_buffer_window_memory = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_session_history, \n",
    "    input_variables=\"input\",\n",
    "    # output_variables=None,\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "274d9784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello John! It's nice to meet you. How can I help you today?\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_buffer_window_memory.invoke(\n",
    "    {\"input\": \"My name is John.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"buffer_window_llm\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "701c4c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I do not have a name. I am a large language model, trained by Google.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_buffer_window_memory.invoke(\n",
    "    {\"input\": \"What is your name?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"buffer_window_llm\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d90cb99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I do not have access to your personal information, so I cannot tell you your name.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM 不會知道 John，因為修剪到最多兩條記憶\n",
    "chain_with_buffer_window_memory.invoke(\n",
    "    {\"input\": \"What is my name:\"},\n",
    "    config={\"configurable\": {\"session_id\": \"buffer_window_llm\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f81fc8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: My name is John.\n",
      "AI: Hello John! It's nice to meet you. How can I help you today?\n",
      "Human: What is your name?\n",
      "AI: I do not have a name. I am a large language model, trained by Google.\n",
      "Human: What is my name:\n",
      "AI: I do not have access to your personal information, so I cannot tell you your name.\n"
     ]
    }
   ],
   "source": [
    "print(get_session_history(\"buffer_window_llm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e527bc9f",
   "metadata": {},
   "source": [
    "### **Summary Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48c3b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(x):\n",
    "    print(f\"Summary: {x['summary']}\", end=f\"\\n\\n{'-' * 20}\\n\\n\")\n",
    "    return x\n",
    "\n",
    "summarizer = GoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n",
    "summary_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Please summarize the following conversation history concisely. If the list is empty, just return nothing.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"), \n",
    "    ]\n",
    ")\n",
    "main_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI assistant. The conversation so far is summarized as follows: {summary}. If the list is empty, it means it's the start of the conversation.\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "summarization_chain = RunnablePassthrough.assign(chat_history=lambda x: x[\"chat_history\"]) | summary_prompt | summarizer\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    input=RunnablePassthrough(func=lambda x: x['input']),\n",
    "    summary=summarization_chain\n",
    ")\n",
    "\n",
    "# {\"input\": ..., \"chat_history\": messeges]}\n",
    "# --> {\"input\": ..., \"summary\": summarized_messeges]}\n",
    "chain = runnable | debug | main_prompt | llm\n",
    "\n",
    "chain_with_summary_memory = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_session_history, \n",
    "    input_variables=\"input\",\n",
    "    # output_variables=None,\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "323ed83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The conversation history is empty.\n",
      "\n",
      "--------------------\n",
      "\n",
      "To evaluate the derivative of $\\sin(\\ln(x^2))$, we need to use the chain rule. The chain rule states that if we have a composite function $f(g(x))$, its derivative is $f'(g(x)) \\cdot g'(x)$.\n",
      "\n",
      "In this case, our outer function is $f(u) = \\sin(u)$ and our inner function is $g(x) = \\ln(x^2)$.\n",
      "\n",
      "First, let's find the derivative of the outer function:\n",
      "$f'(u) = \\frac{d}{du} \\sin(u) = \\cos(u)$\n",
      "\n",
      "Next, let's find the derivative of the inner function, $g(x) = \\ln(x^2)$. We'll need to use the chain rule again for this part, as the argument of the logarithm is itself a function of $x$.\n",
      "Let $h(x) = x^2$. Then $g(x) = \\ln(h(x))$.\n",
      "The derivative of $\\ln(u)$ is $\\frac{1}{u}$.\n",
      "The derivative of $h(x) = x^2$ is $h'(x) = 2x$.\n",
      "\n",
      "So, the derivative of $g(x) = \\ln(x^2)$ is:\n",
      "$g'(x) = \\frac{d}{dx} \\ln(x^2) = \\frac{1}{x^2} \\cdot \\frac{d}{dx}(x^2) = \\frac{1}{x^2} \\cdot 2x = \\frac{2x}{x^2} = \\frac{2}{x}$.\n",
      "\n",
      "Now, we can apply the chain rule to the original function:\n",
      "$\\frac{d}{dx} \\sin(\\ln(x^2)) = f'(g(x)) \\cdot g'(x)$\n",
      "\n",
      "Substitute $g(x) = \\ln(x^2)$ into $f'(u) = \\cos(u)$:\n",
      "$f'(g(x)) = \\cos(\\ln(x^2))$\n",
      "\n",
      "Now multiply by $g'(x)$:\n",
      "$\\frac{d}{dx} \\sin(\\ln(x^2)) = \\cos(\\ln(x^2)) \\cdot \\frac{2}{x}$\n",
      "\n",
      "This can be written as:\n",
      "$\\frac{2 \\cos(\\ln(x^2))}{x}$\n",
      "\n",
      "The final answer is $\\boxed{\\frac{2 \\cos(\\ln(x^2))}{x}}$.\n"
     ]
    }
   ],
   "source": [
    "print(chain_with_summary_memory.invoke(\n",
    "    {\"input\": r\"Evaluate the derivative: \\[ \\frac{d}{dx} \\sin(\\ln(x^2)))\\]\"},\n",
    "    config={\"configurable\": {\"session_id\": \"summary_llm\"}}\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8165390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The user asked to evaluate the derivative of $\\sin(\\ln(x^2))$. The AI used the chain rule twice to find the derivative, first for the outer sine function and then for the inner logarithm function. The final result was $\\frac{2 \\cos(\\ln(x^2))}{x}$.\n",
      "\n",
      "--------------------\n",
      "\n",
      "You asked me to evaluate the derivative of $\\sin(\\ln(x^2))$.\n"
     ]
    }
   ],
   "source": [
    "print(chain_with_summary_memory.invoke(\n",
    "    {\"input\": \"What did i ask you?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"summary_llm\"}}\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
